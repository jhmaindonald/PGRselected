---
title: "Answers to Selected Chapter 8 Exercises"
subtitle: "Tree-based Classification and Regression"
date: "2024-03-15"
output: 
  html_document: default
  pdf_document:
    includes:
      in_header: "preamble.tex"
    latex_engine: xelatex
---

```{css, echo=F}
.colbox {
  padding: 1em;
  background: white;
  color: black;
  border: 2px solid orange;
  border-radius: 10px;
}
.center {
  text-align: left;
}
}
```

```{r, echo=FALSE, include=FALSE}
library(knitr)
opts_chunk$set(fig.align='center', fig.show='hold', size="small", ps=10, 
               out.width="80%", comment=NA, strip.white = TRUE,
                    tidy.opts = list(replace.assign=FALSE))
```

```{r DAAG, echo=TRUE, quiet=TRUE, results='hide'}
library(DAAG)
library(rpart)
```

::: {.colbox data-latex=""}
::: {data-latex=""}
Exercise 1
:::

Refer to the `DAAG::headInjury` data frame.

a.  Use the default setting in `rpart()` to obtain a tree-based model for predicting occurrence of clinically important brain injury, given the other variables.

b.  How many splits gives the minimum cross-validation error?

c.  Prune the tree using the 1 standard error rule.
:::

a.  

```{r ex1a, echo=TRUE, fig.width=5, fig.height=5, ou.width='50%'}
set.seed(29)         ## Gives the results presented here
injury.rpart <- rpart(clinically.important.brain.injury ~ ., 
                      data=headInjury, method="class", cp=0.0001)
plotcp(injury.rpart)
printcp(injury.rpart)
```

The setting `cp=0.0001` was reached after some experimentation.

b.  The minimum cross-validated relative error is for `nsplit=3`, i.e., for a tree size of 4.

c.  The one-standard-error rule likewise chooses `nsplit=3`, with `cp=0.014`. Setting `cp=0.02`, i.e., larger than `cp` for the next smallest number of splits, will prune the tree back to this size. We have

```{r ex1c, echo=TRUE}
injury0.rpart <- prune(injury.rpart, cp=0.02)
```

We plot the tree from (a) that shows the cross-validated relative error, and the tree obtained from (c).

```{r, fig.width=7, fig.height=4, echo=FALSE, out.width='100%'}
par(mfrow=c(1,2))
plotcp(injury.rpart)
plot(injury0.rpart)
text(injury0.rpart)
```

Plots are from the `rpart` analysis of the head injury data: (i) cross-validated relative error versus `cp`; and (ii) the tree obtained in (c).

There can be substantial change from one run to the next.

::: {.colbox data-latex=""}
::: {data-latex=""}
Exercise 2
:::

The data set `mifem` is part of the larger data set in the data frame `monica` that we have included in our *DAAG* package. Use tree-based regression to predict mortality in this larger data set. What is the most immediately striking feature of your analysis? Should this be a surprise?
:::

```{r ex2, echo=TRUE}
monica.rpart <- rpart(outcome ~ ., data=monica, method="class")
```

```{r fig.width=7, fig.height=3.5, echo=TRUE, out.width='100%'}
plot(monica.rpart)
text(monica.rpart)
```

Those who were not hospitalized were very likely to be dead! Check by examining the table:

```{r echo=TRUE}
table(monica$hosp, monica$outcome)
```

::: {.colbox data-latex=""}
::: {data-latex=""}
Exercise 3
:::
> __Note:__ The second sentence of the exercise in the published text is a
mistake.  There is no multiple regression example with this dataset
in Chapter 3, or for that matter elsewhere in the text.  What follows 
is a replacement for the existing exercise.

Use `qqnorm()` to check differences from normality in `nsw74psid1::re78`.
What do you notice?  Use tree-based regression to predict `re78`, and
check differences from normality in the distribution of residuals.  
What do you notice about the tails of the distribution?

a. Use the function `car::powerTransform()` with `family='bcnPower'`
to search for a transformation that will bring the distribution of
`re78` closer to normality. Run summary on the output to get values
(suitably rounded values are usually preferred) of `lambda` and `gamma` 
that you can then supply as arguments to `car::bcnPower()` to obtain
transformed values `tran78` of `re78`.  Use `qqnorm()` with the 
transformed data to compare its distribution with a normal distribution.
The distribution should now be much closer to normal, making the choice
of splits that maximize the between-groups sum-of-squares sums of 
squares about the mean a more optimal procedure.

b. Use tree-based regression to predict `tran78`, and
check differences from normality in the distribution of residuals.
What do you now notice about the tails of the distribution?
What are the variable importance ranks i) if the tree is chosen
that gives the minimum cross-validated error; ii) if the tree
is chosen following the one standard error criterion?  In each
case, calculate the cross-validated relative error.

c. Do a random forest fit to the transformed data, and compare 
the bootstrap error with then cross-validated error from the 
`rpart` fits. 
:::

In order to reproduce the same results as given here, do:

```{r ex3, echo=TRUE}
set.seed(41)
```

Code is:

```{r ex3-initial, echo=TRUE}
re78.rpart <- rpart(re78~., data=DAAG::nsw74psid1, cp=0.001)
plotcp(re78.rpart)
```

It is obvious that `cp=0.0025` will be adequate. At this point, the following is a matter of convenience, to reduce the printed output:

```{r ex3-prune, echo=TRUE}
re78.rpart2 <- prune(re78.rpart, cp=0.0025)
printcp(re78.rpart2)
```
To minimize the cross-validated relative error, prune back
to 9 splits.  Thus e.g., set `cp=0.0038`.

We could prune before comparing the distribution of residuals
to that for a normal distribution.  However, that would make
little difference to the pattern that we now see.  Interested
readers may care to check.

```{r check-resids, echo=TRUE}
res <- resid(re78.rpart2)
qqnorm(res)
```

The distribution of residuals is highly log-tailed, at both ends of the
distribution.  This makes the standard deviation an unsatisfactory measure of variation.  


a. The `car::bcnPower` transformation can be used for an initial
transformation of `re78` that provides a better starting point for the
tree-based regression.

```{r bcn}
set.seed(41)
p2 <- car::powerTransform(nsw74psid1$re78, family='bcnPower') 
summary(p2)
tran78 <- car::bcnPower(nsw74psid1$re78, lambda=0.33, gamma=8000)
tran78a.rpart <- rpart(tran78~., data=DAAG::nsw74psid1[,-10], cp=0.002)
printcp(tran78a.rpart)
```
The minimum cross-validated relative error is at `nsplit=21`. The one standard error limit is 0.477 (=0.45478 +0.022676, round to 3 d.p.). The one standard error rule suggests taking `nsplit=6`.
```{r prune-6}
tran78se.rpart <- prune(tran78a.rpart, cp=0.0065)   ## 1 SE rule
tran78.rpart <- prune(tran78a.rpart, cp=0.0021)     ## Min xval rel error
res <- resid(tran78se.rpart)
qqnorm(res)
```
The long tails have now gone.

Note that the root node error is given as 192.99.
The cross-validated errors for the two trees are, rounded to one decimal place:

* One SE rule: `round(192.99*.46697,1) = 90.1`
* Minimum cross-validated error: `round(192.99*0.45739,1) = 88.3`

c.

```{r rf-tran78}
library(randomForest)
tran78.rf <- randomForest(y=tran78, x=DAAG::nsw74psid1[,-10])
tran78.rf
```

The mean of the squared residuals is given as 
`r tran78.rf$mse[length(tran78.rf$mse)]`.  This is substantially 
larger than either of the cross-validated errors that are given
above from the tree created using `rpart()`. With only a small
number of variables used in tree construction, 
::: {.colbox data-latex=""}
::: {data-latex=""}
Exercise 4
:::
The complete email spam dataset is available from 
https://archive.ics.uci.edu/dataset/94/spambase or from the _bayesreg_ 
package. Carry out a tree-based regression using all 57 available explanatory 
variables.  Determine the change in the cross-validation estimate of 
predictive accuracy.
:::
We set the random number seed to 21, to allow users to reproduce our results. In most other contexts, it will be best not to set a seed.

```{r ex4, echo=TRUE}
data('spambase', package='bayesreg')
## For ease of labeling output, specify short names.  
## Use the following, or do, e.g., nam <- c(paste0('v',1:57), 'yesno')
nam <- c("make", "address", "all", "threeD", "our", "over", "remove", 
"internet", "order", "mail", "receive", "will", "people", "report", 
"addresses", "free", "business", "email", "you", "credit", "your", 
"font", "n000", "money", "hp", "hpl", "george", "n650", "lab", 
"labs", "telnet", "n857", "data", "n415", "n85", "technology", 
"n1999", "parts", "pm", "direct", "cs", "meeting", "original", 
"project", "re", "edu", "table", "conference", "semicolon", "leftparen", 
"leftbrack", "bang", "dollar", "hash", "crl.av", "crl.long", 
"crl.tot", "yesno")
names(spambase) <- nam
```

Now load *rpart* and proceed with the calculations.

```{r ex4-seed, echo=TRUE, eval=TRUE}
set.seed(21)
spam.rpart <- rpart(yesno~., data=spambase, cp=0.0001, method="class")
printcp(spam.rpart)
```

Figure \@ref(spam) shows the plot of cross-validated relative error versus `cp`,
for the full `spam` data set. For making a decision on the size of tree however, 
it is convenient to work from the information given by the function `printcp()`.

```{r, spam, fig.width=4.0, fig.height=3.5, echo=TRUE,  out.width='50%'}
plotcp(spam.rpart)
```
Setting `cp=0.0001` ensures, when the random number seed is set to 21, that the cross-validated relative error reaches a minimum, of 0.1958, at `nsplit=43`. Pruning to get the tree that is likely to have best predictive power can use `cp=0.001`. Adding the SE to the minimum cross-validated relative error gives 0.2. The smallest tree with an SE smaller than this is at `nsplit=36`; setting `cp=0.0012` will give this tree.

Here then are the two prunings:

```{r echo=TRUE}
spam.rpart1 <- prune(spam.rpart, cp=0.001)  # Minimum predicted error
spam.rpart2 <- prune(spam.rpart, cp=0.0012) # 1 SE pruning
```
