[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Answers to selected exercises",
    "section": "",
    "text": "Preface\nThis website makes available solutions to selected exercises from the new Cambridge University Press text, due to appear in published form in May or June 2024:\n\n“A Practical Guide to Data Analysis Using R – An Example-Based Approach”, by John H Maindonald, W John Braun, and Jeffrey L Andrews.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "ch1.html",
    "href": "ch1.html",
    "title": "1  Answers to Selected Chapter 1 Exercises",
    "section": "",
    "text": "library(DAAG)\n\n::: {.colbox data-latex=\"\"}\n::: {data-latex=\"\"}\nExercise 1\n:::\nThe data frame DAAG::orings has details of damage that had occurred in US space shuttle launches prior to the disastrous Challenger launch of January 28, 1986. Observations in rows 1, 2, 4, 11, 13, and 18 were shown in the pre-launch charts used in deciding whether to proceed with the launch, with remaining rows omitted.\n\nCompare plots of Total incidents against Temperature: (i) including only the observations shown in the pre-launch charts; and (ii) using all rows of data. What did the full set of data strongly suggest that was less clear from the plot that showed only the selected rows?\n:::\n\nUse the following to extract rows that hold the data that were presented in the pre-launch charts:\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\norings86 &lt;- DAAG::orings[c(1,2,4,11,13,18), ]\nlibrary(lattice)\ngph1 &lt;- xyplot(Total ~ Temperature, data=orings86, pch=16)\ngph2 &lt;- xyplot(Total ~ Temperature, data=DAAG::orings)\nc(\"Points in pre-launch charts\"=gph1,\"All Points\" = gph2, y.same=TRUE)\n\n\n\n\n\n\n\n:::\nPoints are best shown with filled symbols in the first plot, and with open symbols in the second plot. (Why?)\n\n\nExercise 7\n\nPlot a histogram of the earconch measurements for the possum data. The distribution should appear bimodal (two peaks). This is a simple indication of clustering, possibly due to sex differences. Obtain side-by-side boxplots of the male and female measurements. How do these measurement distributions differ? Can you predict what the corresponding histograms would look like? Plot them to check your answer.\n\n\npossum &lt;- DAAG::possum\npar(mfrow=c(1,2), mar=c(4.1,4.1,1.6,0.6))\nhist(possum$earconch, main=\"\")\nboxplot(earconch ~ sex, data=possum, horizontal=TRUE)\npar(mfrow=c(1,1))\n\n\n\n\n\n\n\n\n\n\nThe left panel shows a histogram of possum ear conch measurements. The right panel shows side by side boxplots of the measurements, one for each sex. A horizontal layout is often advantageous.\nNote the alternative to boxplot() that uses the lattice function bwplot(). Placing sex on the left of the graphics formula leads to horizontal boxplots.\n\nbwplot(sex ~ earconch, data=possum)\n\nThe following uses the lattice function to give side by side histograms:\n\nhistogram(~ earconch | sex, data=possum)\n\n\n\n\n\n\n\n\nDensity plots, in addition to avoiding what has to be a largely arbitrary choice of cutpoints, are easy to overlay.\n\ndensityplot(~earconch, data=DAAG::possum, groups=sex, \n            auto.key=list(columns=2))\n\n\n\n\n\n\n\n\n\n\nExercise 8\n\nFor the data frame ais (DAAG package), draw graphs that show how the values of the hematological measures (red cell count, hemoglobin concentration, hematocrit, white cell count and plasma ferritin concentration) vary with the sport and sex of the athlete.\n\nThe plots that follow show one possibility that gives a relatively compact presentation:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe final 2 in layout=c(2,3,2) spills the panels for the final two measures over onto a second page.\n\n\nExercise 14\n\nAn experimenter intends to arrange experimental plots in four blocks. In each block there are seven plots, one for each of seven treatments. Use the function to find four random permutations of the numbers 1 to 7 that will be used, one set in each block, to make the assignments of treatments to plots.\n\n\nfor(i in 1:4)print(sample(1:7))\n\n[1] 1 2 7 4 6 3 5\n[1] 5 1 7 3 6 2 4\n[1] 6 5 3 7 2 4 1\n[1] 2 4 6 7 3 5 1\n\n## Store results in the columns of a matrix\n## The following is mildly cryptic\nsapply(1:4, function(x)sample(1:7))  \n\n     [,1] [,2] [,3] [,4]\n[1,]    6    7    4    1\n[2,]    7    6    2    2\n[3,]    2    3    3    6\n[4,]    1    4    1    3\n[5,]    5    1    7    7\n[6,]    3    2    6    5\n[7,]    4    5    5    4\n\n\n\n\nExercise 15\n\nThe following data represent the total number of aberrant crypt foci (abnormal growths in the colon) observed in 7 rats that had been administered a single dose of the carcinogen azoxymethane and sacrificed after six weeks:\n87 53 72 90 78 85 83\nEnter these data and compute their sample mean and variance. Is the Poisson model appropriate for these data. To investigate how the sample variance and sample mean differ under the Poisson assumption, repeat the following simulation experiment several times:\n\nx &lt;- rpois(7, 78.3)\nmean(x); var(x)\n\n\n\ny &lt;- c(87, 53, 72, 90, 78, 85, 83)\nc(mean=mean(y), variance=var(y))\n\n     mean  variance \n 78.28571 159.90476 \n\n\nThen try\n\nx &lt;- rpois(7, 78.3)\nc(mean=mean(x), variance=var(x))\n\n    mean variance \n74.71429 70.90476 \n\n\nIt is unusual to get as big a difference between the mean and the variance as that observed for these data, making it doubtful that these data are from a Poisson distribution.\n\n\nExercise 21\n\nThe following code generates random normal numbers with a sequential dependence structure:\n\ny &lt;- rnorm(51) \nydep &lt;- y[-1] + y[-51] \nacf(y, main='A: iid normal values') \nacf(ydep, main='B: Sequentially dependent')\n\nRepeat this several times. There should be no consistent pattern in the acf plot for different iid (independently and identically distributed) random samples y, and a fairly consistent pattern in the acf plot for ydep that reflects the correlation that is introduced by adding to each value of y the next value in the sequence.\n\nThe following should be repeated several times:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn most plots, a lag 1 autocorrelation that is outside of the error bounds shown by the dashed horizontal lines should be evident in the second plot.\n\n\nExercise 22\n\nAssuming that the variability in egg length in the cuckoo eggs data is the same for all host birds, obtain an estimate of the pooled standard deviation as a way of summarizing this variability. [Hint: Remember to divide the appropriate sums of squares by the number of degrees of freedom remaining after estimating the six different means.]\n\n\ncuckoos &lt;- DAAG::cuckoos\nsapply(cuckoos, is.factor)   # Check which columns are factors\n\n length breadth species      id \n  FALSE   FALSE    TRUE   FALSE \n\nspecnam &lt;- levels(cuckoos$species)\nss &lt;- 0\nndf &lt;- 0\nfor(nam in specnam){\n  lgth &lt;- cuckoos$length[cuckoos$species==nam]\n  ss &lt;- ss + sum((lgth - mean(lgth))^2)\n  ndf &lt;- ndf + length(lgth) - 1\n}\nsqrt(ss/ndf)\n\n[1] 0.9051987\n\n\nA more cryptic solution is:\n\ndiffs &lt;- unlist(sapply(split(cuckoos$length, cuckoos$species), \n                function(x)x-mean(x)))\ndf &lt;- unlist(sapply(split(cuckoos$length, cuckoos$species), \n             function(x)length(x) - 1))\nsqrt(sum(diffs^2)/sum(df))",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Answers to Selected Chapter 1 Exercises</span>"
    ]
  },
  {
    "objectID": "ch2.html",
    "href": "ch2.html",
    "title": "2  Answers to Selected Chapter 2 Exercises",
    "section": "",
    "text": "library(DAAG)\n\n\n\nExercise 2\n\nThe table UCBAdmissions was discussed in Subsection 2.2.1. The following gives a table that adds the 2 \\(\\times\\) 2 tables of admission data over all departments:\n\n## UCBAdmissions is in the datasets package\n## For each combination of margins 1 and 2, calculate the sum\nUCBtotal &lt;- apply(UCBAdmissions, c(1,2), sum)\n\nWhat are the names of the two dimensions of this table?\n\nFrom the table UCBAdmissions, create mosaic plots for each faculty separately. (If necessary refer to the code given in the help page for UCBAdmissions.)\n\n\nCompare the information in the table UCBtotal with the result from applying the function mantelhaen.test() to the table UCBAdmissions. Compare the two sets of results, and comment on the difference.\n\n\n\nThe Mantel–Haenzel test is valid only if the male to female odds ratio for admission is similar across departments. The following code calculates the relevant odds ratios:\n\n\n\napply(UCBAdmissions, 3, function(x)\n      (x[1,1]*x[2,2])/(x[1,2]*x[2,1]))\n\nIs the odds ratio consistent across departments? Which department(s) stand(s) out as different? What is the nature of the difference? \\end{enumerate} [For further information on the Mantel-Haenszel test, see the help page for mantelhaen.test.]\n\nUse dimnames(UCBAdmissions)[1:2] to get the names of the first two dimensions, which are Admit and Gender.\n\nFirst note the code needed to give a mosaic plot for the totals; the question does not ask for this. There is an excess of males and a deficit of females in the Admitted category.\n\n\npar(mar=c(3.1,3.1,2.6,1.1))\nUCBtotal &lt;- apply(UCBAdmissions, c(1,2), sum)\nmosaicplot(UCBtotal,col=TRUE)\n\nNow obtain the mosaic plots for each department separately.\n\noldpar &lt;- par(mfrow=c(2,3), mar=c(3.1,3.1,2.6,1), cex.main=0.8)           \nfor(i in 1:6)\n    mosaicplot(UCBAdmissions[,,i], xlab = \"Admit\", ylab = \"Sex\",\n               main = paste(\"Department\", LETTERS[i]), color=TRUE)\n\n\n\n\n\n\n\n\n\n\nMosaic plots are shown for each department separately. The greatest difference in the proportions in the two vertical columns is for Department A.\n\n\n\n\napply(UCBAdmissions, 3, function(x)(x[1,1]*x[2,2])/(x[1,2]*x[2,1]))\n\n        A         B         C         D         E         F \n0.3492120 0.8025007 1.1330596 0.9212838 1.2216312 0.8278727 \n\n\nThe odds ratio (male to female admissions) is much the lowest for Department A.\n\n\nExercise 3\n\nThe following fictitious data is designed to illustrate issues for combining data across tables.\n\n\n       Engineering        Sociology         Sum       \n              Male Female      Male Female Male Female\n                                                      \nAdmit           30     10        15     30   45     40\nDeny            30     10         5     10   35     20\n\n\n       Engineering        Sociology         Sum       \n              Male Female      Male Female Male Female\n                                                      \nAdmit           30     20        10     20   40     40\nDeny            30     10         5     25   35     35\n\n\nThe third dimension in each table is faculty, as required for using faculty as a stratification variable for the Mantel–Haenzel test. From the help page for mantelhaen.test(), extract and enter the code for the function woolf(). Apply the function woolf(), followed by the function mantelhaen.test(), to the data of each of Tables A and B. Explain, in words, the meaning of each of the outputs. Then apply the Mantel–Haenzel test to each of these tables.\n\n\ntabA &lt;- array(c(30, 30, 10, 10, 15, 5, 30, 10),\n                    dim=c(2, 2, 2))\ntabB &lt;- array(c(30, 30, 20, 10, 10, 5, 20, 25),\n                     dim=c(2, 2, 2))\ndimnames(tabA) &lt;- dimnames(tabB) &lt;- \n  list(c('Admit','Deny'), c('Male','Female'), \n       c(\"Engineering\",'Sociology'))\n\n\n     woolf &lt;- function(x) {\n       x &lt;- x + 1 / 2\n       k &lt;- dim(x)[3]\n       or &lt;- apply(x, 3, function(x) (x[1,1]*x[2,2])/(x[1,2]*x[2,1]))\n       w &lt;-  apply(x, 3, function(x) 1 / sum(1 / x))\n       1 - pchisq(sum(w * (log(or) - weighted.mean(log(or), w)) ^ 2), k - 1)\n     }           \nwoolf(tabA)\n\n[1] 0.9695992\n\n\nThe assumption of homogeneity (equal odds ratios for males and females in each of the two departments) appears acceptable.\n\nwoolf(tabB)\n\n[1] 0.04302033\n\n\nThere is evidence of department-specific biases.\n\nmantelhaen.test(tabA)\n\n\n    Mantel-Haenszel chi-squared test without continuity correction\n\ndata:  tabA\nMantel-Haenszel X-squared = 0, df = 1, p-value = 1\nalternative hypothesis: true common odds ratio is not equal to 1\n95 percent confidence interval:\n 0.4565826 2.1901841\nsample estimates:\ncommon odds ratio \n                1 \n\n\nThe estimate of the common odds ratio is 1.\n\nmantelhaen.test(tabB)\n\n\n    Mantel-Haenszel chi-squared test with continuity correction\n\ndata:  tabB\nMantel-Haenszel X-squared = 0.014147, df = 1, p-value = 0.9053\nalternative hypothesis: true common odds ratio is not equal to 1\n95 percent confidence interval:\n 0.448071 1.807749\nsample estimates:\ncommon odds ratio \n              0.9 \n\n\nThe common odds ratio is given as 0.9. However, because the odds ratio is not homogeneous within each of the two departments, this overall figure is, depending on its intended use, misleading.\n\n\nExercise 5\n\nFor constructing bootstrap confidence intervals for the correlation coefficient, it is advisable to work with the Fisher \\(z\\)-transformation of the correlation coefficient. The following lines of R code show how to obtain a bootstrap confidence interval for the \\(z\\)-transformed correlation between chest and belly in the possum data frame. The last step of the procedure is to apply the inverse of the \\(z\\)-transformation to the confidence interval to return it to the original scale. Run the following code and compare the resulting interval with the one computed without transformation. Is the \\(z\\)-transform necessary here?\n\nz.transform &lt;- function(r) .5*log((1+r)/(1-r))\nz.inverse &lt;- function(z) (exp(2*z)-1)/(exp(2*z)+1)\npossum.fun &lt;- function(data, indices) {\n  chest &lt;- data$chest[indices]\n  belly &lt;- data$belly[indices]\n  z.transform(cor(belly, chest))}\npossum.boot &lt;- boot(possum, possum.fun, R=999)\nz.inverse(boot.ci(possum.boot, type=\"perc\")$percent[4:5])\n  # See help(bootci.object).  The 4th and 5th elements of \n  # the percent list element hold the interval endpoints.\n\n\n\nsuppressPackageStartupMessages(library(boot))\n\n\n\n[1] 0.4795131 0.7076292\n\n\n\n\nExercise 6\n\nUse the function rexp() to simulate 100 exponential random numbers with rate .2. Obtain a density plot for the observations. Find the sample mean of the observations. Compare with the result that would be obtained using the normal approximation, i.e., \\(pi/(2*n)\\).\n\n\n\n\n\n\n\n\n\n\nThe density plot is for 100 random values from an exponential distribution with rate = 0.2.\n\n## Code\nz &lt;- rexp(100, .2)\nplot(density(z, from=0), main=\"\")\n\nNotice the use of the argument from=0, to prevent density() from giving a positive density estimate to negative values.\nCompare mean(z) = 5.26 with 1/0.2 = 5.\n\n\nExercise 7\n\nLow doses of the insecticide toxaphene may cause weight gain in rats. A sample of 20 rats are given toxaphene in their diet, while a control group of 8 rats are not given toxaphene. Assume further that weight gain among the treated rats is normally distributed with a mean of 60g and standard deviation 30g, while weight gain among the control rats is normally distributed with a mean of 10g and a standard deviation of 50g. Using simulation, compare confidence intervals for the difference in mean weight gain, using the pooled variance estimate and the Welch approximation. Which type of interval is correct more often?\nRepeat the simulation experiment under the assumption that the standard deviations are 40g for both samples. Is there a difference between the two types of intervals now? Hint: Is one of the methods giving systematically larger confidence intervals? Which type of interval do you think is best?\n\n\n\"Welch.pooled.comparison\" &lt;-\n  function(n1=20, n2=8, mean1=60, mean2=10,\n  sd1=30, sd2=50, nsim=1000) {\n    Welch.count &lt;- logical(nsim)\n    pooled.count &lt;- logical(nsim)\n    Welch.length &lt;- numeric(nsim)\n    pooled.length &lt;- numeric(nsim)\n    mean.diff &lt;- mean1-mean2\n    for (i in 1:1000){\n        x &lt;- rnorm(n1, mean=mean1, sd=sd1)\n        y &lt;- rnorm(n2, mean=mean2, sd=sd2)\n        t1conf.int &lt;- t.test(x, y)$conf.int\n        t2conf.int &lt;- t.test(x, y, var.equal=TRUE)$conf.int\n        t1correct &lt;- (t1conf.int[1] &lt; mean.diff) & (t1conf.int[2] &gt;\n            mean.diff)\n        t2correct &lt;- (t2conf.int[1] &lt; mean.diff) & (t2conf.int[2] &gt;\n            mean.diff)\n        Welch.count[i] &lt;- t1correct\n        pooled.count[i] &lt;- t2correct\n        Welch.length[i] &lt;- diff(t1conf.int)\n        pooled.length[i] &lt;- diff(t2conf.int)\n    }\n    c(\"Welch.proportion.correct\"=mean(Welch.count),\n           \"pooled.proportion.correct\"=mean(pooled.count),\n           \"Welch.length.avg\" = mean(Welch.length),\n           \"pooled.length.avg\" = mean(pooled.length))\n}\nWelch.pooled.comparison()\n\n Welch.proportion.correct pooled.proportion.correct          Welch.length.avg \n                  0.94600                   0.89000                  83.60467 \n        pooled.length.avg \n                 62.33582 \n\nWelch.pooled.comparison(sd1=40, sd2=40)\n\n Welch.proportion.correct pooled.proportion.correct          Welch.length.avg \n                  0.94900                   0.94800                  70.64294 \n        pooled.length.avg \n                 68.09172 \n\n\n\n\nExercise 8\n\n*Experiment with the DAAG::pair65 example and plot various views of the likelihood function, either as a surface using the persp() function or as one-dimensional profiles using the curve() function. Is there a single maximizer: Where does it occur?\n\nFirst, check the mean and the SD.\n\nwith(pair65, heated-ambient)\n\n[1] 19  8  4  1  6 10  6 -3  6\n\nmean(with(pair65, heated-ambient))\n\n[1] 6.333333\n\nsd(with(pair65, heated-ambient))\n\n[1] 6.103278\n\n\nNow create and use a function that calculates the likelihood, given mu and sigma\n\nfunlik &lt;- function(mu, sigma, x=with(pair65, heated-ambient))\n  prod(dnorm(x, mu, sigma))\n\nNext, calculate a vector of values of mu, and a vector of values of sigma\n\nmuval &lt;- seq(from=2, to=12, by=0.5)     # Values about mu=6.33\nsigval &lt;- seq(from=1, to=15, by=0.5)    # Values about mu=6.10\n\nNow calculate an array of loglikelihoods\n\nloglikArray &lt;- function(mu, sigma, d=with(pair65, heated-ambient)){\n  xx &lt;- matrix(0, nrow=length(mu), ncol=length(sigma))\n  for (j in seq(along=sigma)) for (i in seq(along=mu))\n    xx[i,j] &lt;- log(funlik(mu[i], sigma[j], d))\n  xx\n}\nloglik &lt;- loglikArray(mu=muval, sigma=sigval)\n\nPanel B shows a wider range of values of mu, and a narrower range of values of sigma, than in Panel B:\n\npar(mfrow=c(1,2))\npersp(x=muval, y=sigval, loglik, main=\"A: Initial choice of value ranges\")\nmuval &lt;- seq(from=-1, to=14, by=0.5)\nsigval &lt;- seq(from=3, to=12, by=0.2)\nloglik &lt;- loglikArray(mu=muval, sigma=sigval)\npersp(x=muval, y=sigval, loglik, main=\"B: Adjusted value ranges\")\n\n\n\n\n\n\n\n\nTry also\n\npar(mfrow=c(1,2))\ncontour(muval, sigval, loglik)\nfilled.contour(muval, sigval, loglik)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 9\n\n*Suppose the mean reaction time to a particular stimulus has been estimated in several previous studies, and it appears to be approximately normally distributed with mean 0.35 seconds with standard deviation 0.1 seconds. On the basis of 10 new observations, the mean reaction time is estimated to be 0.45 seconds with an estimated standard deviation of 0.15 seconds. Based on the sample information, what is the maximum likelihood estimator for the true mean reaction time? What is the Bayes’ estimate of the mean reaction time.\n\nFollowing Section 2.9.1 the posterior density of the mean is normal with mean \\[ \\frac{n \\bar{y} + \\mu_0 \\sigma^2/\\sigma_0^2}{n + \\sigma^2/\\sigma_0^2} \\] and variance \\[ \\frac{\\sigma^2}{n + \\sigma^2/\\sigma_0^2}\\] where, here \\[  \\mu_0 = 0.35, \\sigma_0 = 0.1, \\quad \\bar{y} = 0.45, n = 10,\n\\sigma = 0.15 \\] Thus the posterior mean and variance of the mean are:\n\nprint(c(mean = (10 * 0.45 + 0.35 * 0.15^2/0.1^2)/(10 + 0.15^2/0.1^2)))\n\n     mean \n0.4316327 \n\nprint(c(variance = 0.1^2/(10 + 0.15^2/0.1^2)))\n\n    variance \n0.0008163265 \n\n\nThe posterior mean is the Bayes’ estimate of the mean.\n\n\nExercise 10\n\nUse the robust regression function MASS::rlm() to fit lines to the data in elastic1 and elastic2. Compare the results with those from use of lm(). Compare regression coefficients, standard errors of coefficients, and plots of residuals against fitted values.\n\nThe required regressions are as follows:\n\ne1.lm &lt;- lm(distance ~ stretch, data=elastic1)\ne2.lm &lt;- lm(distance ~ stretch, data=elastic2)\n\nThe fitted values and standard errors of the fits are then:\n\npredict(e1.lm, se.fit=TRUE)\n\n$fit\n       1        2        3        4        5        6        7 \n183.1429 235.7143 196.2857 209.4286 170.0000 156.8571 222.5714 \n\n$se.fit\n[1]  6.586938 10.621119  5.891537  6.586938  8.331891 10.621119  8.331891\n\n$df\n[1] 5\n\n$residual.scale\n[1] 15.58754\n\n\nThe standard errors are somewhat smaller for the second data set than for the first\nThe robust regression fits can be obtained as follows:\n\nsuppressPackageStartupMessages(library(MASS))\ne1.rlm &lt;- rlm(distance ~ stretch, data=elastic1)\ne2.rlm &lt;- rlm(distance ~ stretch, data=elastic2)\n\nThe residual plots can be obtained for rlm() in the same was as for . It may however be more insightful to overlay the rlm() plots on the lm() plots.\n\npar(mfrow=c(1,2))\nplot(e1.lm, which=1, add.smooth=FALSE)\npoints(resid(e1.rlm) ~ fitted(e1.rlm), col=2, pch=2)\nplot(e2.lm, which=1, add.smooth=FALSE)\npoints(resid(e2.rlm) ~ fitted(e2.rlm), col=2, pch=2)\n\n\n\n\n\n\n\n\nThe figure shows overlaid plots of residuals versus fitted values, for the dataframes elastic1 (left panel) and elastic2 (right panel). Circles are for the lm fit and triangles for the rlm fit.\nFor comparison purposes, we include residual plots for the ordinary regression fits. Note, in particular, how the robust regression has reduced the weight of the outlying observation in the first data set. The residual at that point is larger than it was using ordinary least-squares. The residual plots for the ordinary and robust fits are very similar for the second data set, since there are no outlying observations.\nAs can be seen in the summaries below, the ordinary and robust fits for the first data set give quite different estimates of the slope and intercept. The robust fit is more in line with both sets of results obtained for the second data set.\nNote also the downward effect of the robust regression on the residual standard error. This is again due to the down-weighting of the outlying observation.\nFor further details, run the following code:\n\nsummary(e1.rlm)\nsummary(e1.lm)\nsummary(e2.rlm)\nsummary(e2.lm)\n\n\n\nExercise 11\n\nIn the data set pressure (datasets), examine the dependence of pressure on temperature.\n[The relevant theory is that associated with the Claudius-Clapeyron equation, by which the logarithm of the vapor pressure is approximately inversely proportional to the absolute temperature. For further details of the Claudius-Clapeyron equation, search on the internet, or look in a suitable reference text.]\n\nThe following fits the Claudius-Claperon equation\n\npar(mfrow=c(1,2))\npressure$K &lt;- pressure$temperature+273\nplot(log(pressure) ~ I(1/K), data=pressure)\np.lm &lt;- lm(log(pressure) ~  I(1/K), data=pressure)\ntitle(main='A: Points, with fitted line')\nplot(p.lm, which=1, main='B: Residuals vs fitted values')\n\n\n\n\n\n\n\n\nLook also at the adjusted R-squared statistic:\n\nprint(summary(p.lm)$adj.r.squared, digits=6)\n\n[1] 0.999888\n\n\nThe plot in Panel A, and the adjusted R-squared statistic, show a very close fit. Nonetheless, Panel B shows systematic departures from the fitted line.\nPanel A in the figure below shows large departures from the line that is fitted on a logarithmic scale at the lowest temperatures, where the pressure is also lowest. Working with log(pressure) exaggerates the weight given to low values for pressure, relative to high values. The linear equation \\[ \\log(pressure) = a + \\frac{b}{K} \\] can be rewritten: \\[\npressure = \\mbox{exp}(a)\\mbox{exp}(\\frac{b}{K})\n\\] The function nls() can then be used for a fit of this nonlinear version of the model, thus giving equal weight to all values of pressure. Panel B shows residuals whose scatter is then much more similar across all values of pressure.\n\npar(mfrow=c(1,2))\nplot(resid(p.lm) ~ K, data=pressure)\ntitle(main='A: Residuals from lm fit, vs `K`')\nab &lt;- coef(p.lm)\np.nls &lt;- nls(pressure ~ A*exp(b/K), data=pressure, start=list(A=exp(ab[1]),b=ab[2]))\nplot(resid(p.nls) ~ fitted(p.nls))\ntitle(main='B: Residuals from nls fit')\n\n\n\n\n\n\n\n\nThe use of nls() for nonlinear least squares is discussed in Subsection 3.8.4.\n\n\nExercise 15\n\n*A Markov chain is a data sequence which has a special kind of dependence. For example, a fair coin is tossed repetitively by a player who begins with $2. If `heads’ appear, the player receives one dollar; otherwise, she pays one dollar. The game stops when the player has either $0 or $5. The amount of money that the player has before any coin flip can be recorded – this is a Markov chain. A possible sequence of plays is as follows:\nPlayer's fortune:  2  1  2  3  4  3  2  3  2  3  2  1  0 \nCoin Toss result:  T  H  H  H  T  T  H  T  H  T  T  T \nNote that all we need to know in order to determine the player’s fortune at any time is the fortune at the previous time as well as the coin flip result at the current time. The probability of an increase in the fortune is .5 and the probability of a decrease in the fortune is .5. Such transition probabilities are usually summarized in a transition matrix:\n\\[ P = \\left[ \\begin{array}{c c c c c c} 1 & 0 & 0 & 0 & 0 & 0 \\\\\n                                 .5 & 0 & .5 & 0 & 0 & 0 \\\\\n                                  0 & .5 & 0 & .5 & 0 & 0 \\\\\n                                  0 & 0 & .5 & 0 & .5 & 0 \\\\\n                                  0 & 0 & 0 & .5 & 0 & .5 \\\\\n                                  0 & 0 & 0 & 0 & 0 & 1\\\\\n                                  \\end{array} \\right]\\]\nThe \\((i,j)\\) entry of this matrix refers to the probability of making a change from the value \\(i\\) to the value \\(j\\). Here, the possible values of \\(i\\) and \\(j\\) are \\(0, 1, 2, \\ldots, 5\\). According to the matrix, there is a probability of 0 of making a transition from $2 to $4 in one play, since the (2,4) element is 0; the probability of moving from $2 to $1 in one transition is 0.5, since the (2,1) element is 0.5.\nThe following function can be used to simulate \\(N\\) values of a Markov chain sequence, with transition matrix \\(P\\):\n\nMarkov &lt;- function (N=100, initial.value=1, P)\n{\n    X &lt;- numeric(N)\n    X[1] &lt;- initial.value + 1\n    n &lt;- nrow(P)\n    for (i in 2:N){\n    X[i] &lt;- sample(1:n, size=1, prob=P[X[i-1],])}\n    X - 1\n}\n## Set `stopval=c(0,5)` to stop when the player's fortune is $0 or $5\n\nSimulate 15 values of the coin flip game, starting with an initial value of $2.\n\nCode which may be used is:\n\nP &lt;- matrix(c(1, rep(0,5), rep(c(.5,0,.5, rep(0,4)),4), 0,1),\n            byrow=TRUE,nrow=6)\nMarkov(15, 2, P)\n\n\n\nExercise 15 – additional exercises\n\n\nSimulate 100 values of the Markov chain which has the following transition matrix. Save the result to a vector and use ts.plot() to plot the sequence. \\[ P = \\left[\n\\begin{array}{rrrrrr}\n0.10 & 0.90 & 0.00 & 0.00 & 0.00 & 0.00 \\\\\n0.50 & 0.00 & 0.50 & 0.00 & 0.00 & 0.00 \\\\\n0.00 & 0.50 & 0.00 & 0.50 & 0.00 & 0.00 \\\\\n0.00 & 0.00 & 0.50 & 0.00 & 0.50 & 0.00 \\\\\n0.00 & 0.00 & 0.00 & 0.50 & 0.00 & 0.50 \\\\\n0.00 & 0.00 & 0.00 & 0.00 & 1.00 & 0.00 \\\\\n\\end{array} \\right] \\]\nNow simulate 1000 values from the above Markov chain, and calculate the proportion of times the chain visits each of the states. It can be shown, using linear algebra, that in the long run, this Markov chain will visit the states according to the following stationary distribution\n\n 0         1         2         3         4         5\n 0.1098901 0.1978022 0.1978022 0.1978022 0.1978022 0.0989011\nThere is a result called the ergodic theorem which allows us to estimate this distribution by simulating the Markov chain for a long enough time. Compare your calculated proportions with above theoretical proportions. Repeat the experiment using 10000 simulated values; the calculated proportions should be even closer to the theoretically predicted proportions in that case.\nc. Simulate 100 values of the Markov chain which has the following transition matrix. Plot the sequence. Compare the results when the initial value is 1 with when the initial value is 3, 4, or 5. [When the initial value is 0 or 1, this Markov chain wanders a bit before settling down to its stationary distribution which is concentrated more on the values 4 and 5. This wandering period is sometimes called `burn-in’.] \\[ P = \\left[\n\\begin{array}{llllll}\n0.50 & 0.50 & 0    & 0    & 0    & 0    \\\\\n0.50 & 0.45 & 0.05 & 0    & 0    & 0    \\\\\n0    & 0.01 & 0    & 0.90 & 0.09 & 0    \\\\\n0    & 0    & 0.01 & 0.40 & 0.59 & 0    \\\\\n0    & 0    & 0    & 0.50 & 0    & 0.50 \\\\\n0    & 0    & 0    & 0    & 0.50 & 0.50 \\\\\n\\end{array} \\right] \\]\n\nHere is code that may be used for these calculations.\n\n\n\n\nPb &lt;- matrix(c(0.10,0.90,0.00,0.00,0.00,0.00,\n               0.50,0.00,0.50,0.00,0.00,0.00,\n               0.00,0.50,0.00,0.50,0.00,0.00,\n               0.00,0.00,0.50,0.00,0.50,0.00,\n               0.00,0.00,0.00,0.50,0.00,0.50,\n               0.00,0.00,0.00,0.00,1.00,0.00), \n             byrow=TRUE, nrow=6)\nxb &lt;- Markov(100, 1, Pb)\nxb\nts.plot(xb)\n\n\n\n\n\nxc &lt;- Markov(1000, 1, Pb)\ntable(xb)/1000  # one of several ways to calculate the proportions\nxc\nxc2 &lt;- Markov(10000, 1, Pb)\ntable(xc2)/10000\n\n\n\n\n\nPd &lt;- matrix(c(0.50, 0.50, 0, 0, 0, 0,\n               0.50, 0.45, 0.05, 0, 0, 0,\n               0, 0.01, 0, 0.90, 0.09, 0,\n               0, 0, 0.01, 0.40, 0.59, 0,\n               0, 0, 0, 0.50, 0, 0.50,\n               0, 0, 0, 0, 0.50, 0.50), \n             nrow=6, byrow=TRUE)\nThe following function may be helpful, in examining results.\n`plotmarkov` &lt;-\n  function(n=10000, start=1, window=100, transition=Pd){\n    xc2 &lt;- Markov(n, start, transition)\n    z4 &lt;- as.integer(xc2==4)\n    z5 &lt;- as.integer(xc2==5)\n    mav4 &lt;- rollmean(z4,window)\n    mav5 &lt;- rollmean(z5,window)\n    df &lt;- data.frame(av4=mav4, av5=mav5, x=rep(1:1000, length=length(mav4)),\n                     gp=(0:(length(mav4)-1))%/%1000)\n    print(xyplot(av4+av5 ~ x | gp, data=df, layout=c(1,10), type=\"l\",\n                 par.strip.text=list(cex=0.65)))\n  }\n## Use thus\nlibrary(zoo)  # Use rollmean() [moving average] from zoo\nlibrary(lattice)\nplotmarkov(start=1)\nplotmarkov(start=4)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Answers to Selected Chapter 2 Exercises</span>"
    ]
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "3  Summary",
    "section": "",
    "text": "In summary, this book has no content whatsoever.\n\n1 + 1\n\n[1] 2",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Summary</span>"
    ]
  }
]